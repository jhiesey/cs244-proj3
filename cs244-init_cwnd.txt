Introduction

    In this paper, we measure the effect of increasing size of the TCP initial congestion window.  In doing so, we attempt to replicate and extend some of the results of Dukkipati, et.al.[1]
    In the original paper, the authors propose increasing TCP's initial congestion window to decrease the latency of HTTP requests from a browser to a Web server.  Often, HTTP creates a connection and then requests a small object (on the order of tens of a few kilobytes), and often closes the connection afterwards.
    The TCP slow start algorithm increases the congestion window size, which represents the maximum amount of data that can be outstanding before an ACK, by one segment size per ACK received, starting with the initial congestion window size (init_cwnd).  Since Web connections tend to be short, they typically close before leaving the slow start phase, making init_cwnd have a large effect on how many round trip times a HTTP request/response requires.  The original paper describes, in some depth, why increasing init_cwnd can improve the performance of the Web.
    We have decided to replicate a number of the experiments from the original paper, as well as to investigate some potential problems with larger initial congestion windows.  To do so, we utilize MiniNet, a network emulator that runs on top of a single Linux host.  By building topologies with controlled link bandwidth, latency, and other properties, we can evaluate the effect of changing init_cwnd in a controlled environment.
    First, we will measure the effect of the increase in congestion window size has on the latency of small, fixed-size web requests for links with various RTTs and bandwidths.  We will explore whether these results agree with those in the original paper, and determine whether an even larger congestion window size may help to a greater degree.  In addition, we will explore the limitations MiniNet imposes on these measurements.
    Second, we will attempt to evaluate the potential negative impact of larger init_cwnd.  The Dukkipati et.al. paper[1] discusses the potential for larger initial congestion windows to cause more frequent packet loss due to buffers overflowing.  Unfortunately, the authors of that paper only attempted to measure this effect using a small amount of data.  We attempt to quantify this effect in MiniNet by opening many TCP connections over a bottleneck link and evaluating the goodput of this network for various init_cwnd sizes.
    
Experimental Setup
[Describe the use of MiniNet]

Evaluation on Uncongested Links
[A comparison to the original paper's data]

Evaluation on Congested Links
[New data on the effects a larger init_cwnd has on overall throughput]

Conclusions
[Our conclusions]

References
[Cite the original paper, and anything else we use]
