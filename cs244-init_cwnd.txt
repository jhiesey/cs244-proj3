Introduction

    In this paper, we measure the effect of increasing size of the TCP initial congestion window.  In doing so, we attempt to replicate and extend some of the results of Dukkipati, et.al.[1]
    In the original paper, the authors propose increasing TCP's initial congestion window to decrease the latency of HTTP requests from a browser to a Web server.  Often, HTTP creates a connection and then requests a small object (on the order of tens of a few kilobytes), and often closes the connection afterwards.
    The TCP slow start algorithm increases the congestion window size, which represents the maximum amount of data that can be outstanding before an ACK, by one segment size per ACK received, starting with the initial congestion window size (init_cwnd).  Since Web connections tend to be short, they typically close before leaving the slow start phase, making init_cwnd have a large effect on how many round trip times a HTTP request/response requires.  The original paper describes, in some depth, why increasing init_cwnd can improve the performance of the Web.
    We have decided to replicate a number of the experiments from the original paper, as well as to investigate some potential problems with larger initial congestion windows.  To do so, we utilize MiniNet, a network emulator that runs on top of a single Linux host.  By building topologies with controlled link bandwidth, latency, and other properties, we can evaluate the effect of changing init_cwnd in a controlled environment.
    First, we will measure the effect of the increase in congestion window size has on the latency of small, fixed-size web requests for links with various RTTs and bandwidths.  We will explore whether these results agree with those in the original paper, and determine whether an even larger congestion window size may help to a greater degree.  In addition, we will explore the limitations MiniNet imposes on these measurements.
    Second, we will attempt to evaluate the potential negative impact of larger init_cwnd.  The Dukkipati et.al. paper[1] discusses the potential for larger initial congestion windows to cause more frequent packet loss due to buffers overflowing.  Unfortunately, the authors of that paper only attempted to measure this effect using a small amount of data.  We attempt to quantify this effect in MiniNet by opening many TCP connections over a bottleneck link and evaluating the goodput of this network for various init_cwnd sizes.
    
Experimental Setup
For all of our experiments we use the general topology shown below implemented in mininet. The only link with realistic parameters is the bottleneck link between the two switches in the center. The links connecting hosts to the two center switches are modeled with infinite bandwidth and zero latency. Each host on the left communicates with exactly one server on the right. The clients on the left run a curl command to download a 30 kB (30 thousand byte) file from their respective servers. We measure the latency as reported by curl.

\image{topology.png}

To replicate the main results of [1], we use only one client/server pair and set the delay of the bottleneck link to various values to change the end-to-end RTT, our independent variable. For each RTT we compare the latency of transferring the 30 kB file for the two initial congestion window sizes (4, the default and 10, the proposed new value).

For our second experiment, we simply set the loss rate of the link to 5% and otherwise perform the same experiment as in the first part.
[Describe the use of MiniNet]

Evaluation on Uncongested Links
[A comparison to the original paper's data]

Evaluation on Congested Links
[New data on the effects a larger init_cwnd has on overall throughput]

Conclusions
[Our conclusions]

References
[Cite the original paper, and anything else we use]
