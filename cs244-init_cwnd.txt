Introduction

    In this paper, we measure the effect of increasing size of the TCP initial congestion window.  In doing so, we attempt to replicate and extend some of the results of Dukkipati, et.al.[1]
    In the original paper, the authors propose increasing TCP's initial congestion window to decrease the latency of HTTP requests from a browser to a Web server.  Often, HTTP creates a connection and then requests a small object (on the order of tens of a few kilobytes), and often closes the connection afterwards.
    The TCP slow start algorithm increases the congestion window size, which represents the maximum amount of data that can be outstanding before an ACK, by one segment size per ACK received, starting with the initial congestion window size (init_cwnd).  Since Web connections tend to be short, they typically close before leaving the slow start phase, making init_cwnd have a large effect on how many round trip times a HTTP request/response requires.  The original paper describes, in some depth, why increasing init_cwnd can improve the performance of the Web.
    We have decided to replicate a number of the experiments from the original paper, as well as to investigate some potential problems with larger initial congestion windows.  To do so, we utilize MiniNet, a network emulator that runs on top of a single Linux host.  By building topologies with controlled link bandwidth, latency, and other properties, we can evaluate the effect of changing init_cwnd in a controlled environment.
    First, we will measure the effect of the increase in congestion window size has on the latency of small, fixed-size web requests for links with various RTTs and bandwidths.  We will explore whether these results agree with those in the original paper, and determine whether an even larger congestion window size may help to a greater degree.  In addition, we will explore the limitations MiniNet imposes on these measurements.
    Second, we will attempt to evaluate the potential negative impact of larger init_cwnd.  The Dukkipati et.al. paper[1] discusses the potential for larger initial congestion windows to cause more frequent packet loss due to buffers overflowing.  Unfortunately, the authors of that paper only attempted to measure this effect using a small amount of data.  We attempt to quantify this effect in MiniNet by opening many TCP connections over a bottleneck link and evaluating the goodput of this network for various init_cwnd sizes.
    
Experimental Setup

For all of our experiments we use the general topology shown below implemented in mininet. The only link with realistic parameters is the bottleneck link between the two routers (labeled r1 and r2) in the center of the diagram. The links connecting hosts to their router are modeled with infinite bandwidth and zero latency. In addition, we use a simple identity traffic matrix in our experiments. Thus, each client on the left communicates with exactly one server on the right. Client 1 requests from server 1, client 2 from server 2, and so on. The clients on the left run a curl command to download a 30 kB (30 thousand byte) file from their respective servers. We measure the latency as reported by curl as our dependent variable.

\image{topology.png}

To replicate the main results of [1], we use only one client/server pair (N = 1) and set the delay of the bottleneck link to various values to change the end-to-end RTT, our independent variable. For each RTT we compare the latency of transferring the 30 kB file for the two initial congestion window sizes (3, the default, and 10, the proposed new value).

For our second experiment, we simply set the loss rate of the link to 5% and otherwise perform the same experiment as in the first part.

In the third and final experiment, we TODO

For these experiments to work, we must use the ip command to change the initial congestion window on the mininet hosts. The command to change the initial congestion window is "ip route change default via <gateway> dev eth0 initcwnd <iw>
". It is important to note that linux kernel versions 2.6.39 through 3.2 have a bug that makes this command not work. Thus, for our experiments we used linux kernel version 3.3 compiled for Amazon AWS in a Ubuntu 12.04 LTS distribution. We ran then ran all of our experiments on Amazon's EC2 service with this setup.

Evaluation on Uncongested Links

In our first experiment, we were able to reproduce fairly closely the results that were obtained in the original paper.

\image{fig5_original.png} \image{fig5_repro.png}

<Comment on differences between orig and repro>

In our second experiment, we observed that TODO

In our third experiment, 

[A comparison to the original paper's data]

Evaluation on Congested Links
[New data on the effects a larger init_cwnd has on overall throughput]

Conclusions
[Our conclusions]

References
Nandita Dukkipati et al.,
\emph{An Argument for Increasing TCPâ€™s Initial Congestion Window}
ACM SIGCOMM, July 2010

Nandita Dukkipati et al.,
\emph{Increasing TCP initial window}
78th IETF, Maastricht
Slides at: http://www.ietf.org/proceedings/78/slides/iccrg-3.pdf

The source code for this project can be found at: TODO
